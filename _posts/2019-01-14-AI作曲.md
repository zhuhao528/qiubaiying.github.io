---
layout:     post
title:      C-RNN-GAN：连续对抗神经网络
subtitle:   人工智能作曲
date:       2019-01-14
author:     Neil
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 作曲
---

#Introduction  
生成对抗网络（GAN）是一类神经网络架构，旨在生成真实数据[Goodfellow et al。，2014]。该方法涉及训练具有冲突目标的两个神经模型，一个发生器（G）和一个鉴别器（D），迫使彼此改进。生成器尝试生成看起来真实的样本，并且鉴别器尝试区分生成的样本和实际数据。在这项工作中，我们研究了对连续数据的顺序模型使用对抗训练的可行性，并在免费提供的midi文件中使用古典音乐进行评估。递归神经网络（RNN）通常用于模拟数据序列。通常使用最大似然标准训练这些模型。例如。对于语言建模，训练他们在序列中的任何点预测下一个标记，即在给定前一个标记序列的情况下对下一个标记的条件概率进行建模。通过从这个条件分布中抽样，可以生成相当逼真的序列[Graves，2013]。

RNN已被用于模拟音乐[Eck和Schmidhuber，2002，Nicolas Boulanger -Lewandowski，2012，Yu等人，2016]，但据我们所知，他们总是使用符号表示。使用RNN进行音乐生成的工作包括[Eck和Schmidhuber，2002]，使用25个离散音调值建模布鲁斯歌曲，以及[Nicolas Boulanger-Lewandowski，2012]，将RNN与受限制的Boltzmann机器相结合，代表88种不同的音调。Yu等人[2016]训练RNN进行对抗性训练，应用梯度方法来应对他们所采用的象征性表现的离散性质。与此相反，我们的工作使用频率，长度，强度和时间的实值连续四元组来表示音调。这允许我们使用标准反向传播来端对端地训练整个模型。我们提出了一种递归神经网络架构，C-RNN-GAN（连续RNN-GAN），用对抗训练，以模拟序列的整个联合概率，并能够生成数据序列。我们的系统通过对midi格式的古典音乐序列进行训练来演示，并使用诸如音阶一致性和音调范围等指标进行评估。
![](https://ws4.sinaimg.cn/large/006tNc79ly1fz64j27nugj30k009f3z2.jpg)

C-RNN-GAN: A continuous recurrent network with adversarial training

提出的模型是具有对抗训练的递归神经网络。两种不同的深度递归神经模型，即发生器（G）和鉴别器（D）。训练生成器以生成与实际数据无法区分的数据，同时训练鉴别器以识别生成的数据。训练成为零和游戏，纳什均衡是当发生器产生鉴别器无法从实际数据中分辨出来的数据时。我们定义了以下损失函数L D和L G：

![](https://ws1.sinaimg.cn/large/006tNc79ly1fz64ibqycrj30fv04l0ss.jpg)
(where z (i) is a sequence of uniform random vectors in [0, 1] k , and x (i) is a sequence from thetraining data. k is the dimensionality of the data in the random sequence.)

G中每个单元的输入是一个随机向量，与前一个单元的输出连接。当训练RNN作为语言模型时，从前一个单元输出输出是常见的做法[Mikolov et al。，2010]，并且也被使用在音乐作曲[Eck and Schmidhuber，2002]。鉴别器由双向经常性网络组成，允许它在两个方向上考虑其决策的背景。在这项工作中，使用的循环网络是长期短期记忆（LSTM）[Schmidhuber和Hochreiter，1997]。它有一个带有门的内部结构，有助于消除梯度问题，并学习更长的依赖性[Hochreiter，1998，Bengio et al。，1994]。

在这项工作中，我们开始评估使用生成对抗模型来学习古典音乐背后的生成分布的可行性。受到用于在数字乐器之间传递信号的古老MIDI格式的启发，我们在每个数据点处使用四个实值标量对信号进行建模：音调长度，频率，强度和前一音调所花费的时间。以这种方式对数据建模允许网络表示多声和弦（两个音调之间的时间为零）。为了评估其对复音的影响，我们还尝试了将最多三个音调表示为G中每个LSTM单元的输出（对D的相应修改）。然后，如上所述，每个音调用其自己的四元组值表示。我们将此版本称为C-RNN-GAN-3。与MIDI格式类似，没有音调由零强度输出表示。

#Experimental setup

模型布局细节：G和D中的LSTM网络都具有深度2，每个LSTM单元具有350个内部（隐藏）单元。 D具有双向布局，而G是单向的。来自D中的每个LSTM单元的输出被馈送到完全连接的层，其中权重在时间步长上共享，然后每个单元的sigmoid输出被平均。

Baseline：我们的Baseline是一个类似于我们的生成器的循环网络，但训练完全以预测循环中每个点的下一个音调。

数据集：训练数据是以midi格式的音乐文件形式从网上收集的，包含着名的古典音乐作品。 每个midi事件被加载并与其持续时间，音调，强度（速度）以及自上一音调开始以来的时间一起保存。音调数据在内部用相应的声音频率表示。所有数据归一化为每秒384点的刻度分辨率。 该数据包含来自160位不同古典音乐作曲家的3697个m​​idi文件。源代码可在Github 上获得，包括从不同网站下载本研究中使用的所有数据的实用程序。

Training：使用反向传播（BPTT）和小批量随机梯度下降。学习率设置为0.1，并且我们将L2正则化应用于G和D中的权重。该模型预训练6个epochs，平方误差损失以预测训练序列中的下一个事件。就像在对抗设置中一样，每个LSTM单元的输入是随机向量v，与前一时间步的输出连接。 v均匀分布在 [0,1]^ k 中，并且k被选择为每个音调中的特征数量。在预训练期间，我们使用序列长度的模式，从短序列开始，从训练数据中随机采样，最终用越来越长的序列训练模型。

[github](https://github.com/olofmogren/c-rnn-gan)  

在对抗训练期间，D可能变得太强，导致渐变无法用于改善G.当网络初始化而没有预训练时，这种效果尤为明显。出于这个原因，我们应用freezing[Salimans et al。，2016]，这意味着当训练损失小于G的训练损失的70％时停止D的更新。当G变得太强时，我们做相应的事情。我们还采用特征匹配[Salimans et al。，2016]，这是一种鼓励G更大变化的方法，并避免通过更换标准生成器损失L G过拟合到当前判别器。

通常，生成器的目标是最大化判别器所产生的误差，但是通过特征匹配，目标是在与实际数据匹配的判别器的某一水平上产生内部表示。我们从D中最终逻辑分类层之前的最后一层选择表示R，并为G定义新目标LG`.

![](https://ws4.sinaimg.cn/large/006tNc79ly1fz64h2nenwj30ar01zmx1.jpg)
![](https://ws4.sinaimg.cn/large/006tNc79ly1fz64glhvu7j30pu0kwgsf.jpg)

图3：从评估模型中生成音乐的统计信息。C-RNN-GaN（a）随着训练的进行而产生复杂的音乐。使用的unique tones的数量有一个模糊增加的趋势，而尺度一致性似乎稳定在十或十五个epochs之后。3-tone重复在前25个epochs有增加的趋势，然后停留在相当低的水平上，似乎与所使用的音调的数量相关。baseline模型（B）没有达到相同的变化水平。所使用的unique tones的数量总是低得多，而尺度一致性似乎类似于C-RNN-GaN。音调跨度跟随 比C-RN-GaN更接近的unique tones的数目，这表明基线在使用的音调中具有较少的可变性。C-RNN-GAN-3（C）获得更高的复调分数，与C-RNN-GaN和baseline相反。在到达大约50至55epochs的多个零值输出的状态之后，C-RNN-GAN-3在音调跨度、unique tones的数目、强度跨度和3-tone重复上达到更高的值。在（d）中，人们可以看到真正的音乐具有与生成音乐相似的强度跨（intensity span），规模一致性稍高，但变化也更大。复调得分与C-RNN-GAN-3相似。3-tone的重复要高得多，但由于歌曲的长度不同，很难进行比较。计数通过L R／L G进行归一化，其中L R是真实音乐的长度，L G是生成音乐的长度。

Evaluation
使用对生成的输出的多个测量来完成对C-RNN-GAN的评估。

Polyphony，测量同时播放（至少）两个音调的频率（它们的开始时间完全相同）。请注意，这是一个相当有限的指标，因为它可以为具有同样音调但不会在同一时间开始的音乐提供低分。

Scale consistency是通过计算作为标准尺度的一部分的音调的分数来计算的，并报告最佳匹配的数量。

计算Repetitions的短子序列数，给出样本中循环多少的分数。此指标仅考虑音调及其顺序，而不考虑其时间。

Tone span是样本中最低和最高音调之间的半音调步数。实现了一个工具来计算这些估计，这些工具可以在Github上获得，同时还有本工作中使用的所有源代码。

[github](https://github.com/olofmogren/c-rnn-gan)  

#Discussion and conclusions

在本文中，我们提出了一种连续数据的递归神经模型，使用基于生成对抗网络的方法进行训练。虽然需要进行更多实验，但我们认为结果很有希望。我们已经注意到，对抗性训练有助于递归神经网络产生的音乐在所使用的音调数量和播放音调的强度范围内变化更大。通过人为判断，生成的音乐还不能与训练数据中的音乐进行比较。其原因仍有待探索。然而，在评估中（参见图3），可以看出使用C-RNN-GAN生成的音乐的分数与使用baseline生成的音乐的分数相比，与真实音乐的分数更相似。生成的音乐是复音的，但在我们的实验评估中的复音乐谱中，测量两个音调在同一时间播放的频率，C-RNN-GAN得分低。允许每个LSTM单元同时输出多达三个音调，这导致模型在复音时得分更高。人们可以在生成的样本中听到，虽然时间可以在不同样本之间变化很大，但是在一个轨道内通常相当一致，从而在所生成的歌曲中给出节奏的感觉。

参考地址  
[论文地址](https://arxiv.org/abs/1611.09904)  
[github地址](https://github.com/olofmogren/c-rnn-gan)  
[知乎地址](https://www.zhihu.com/question/22213757)